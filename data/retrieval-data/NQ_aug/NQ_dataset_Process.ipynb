{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364e00e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import gzip\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import Any, List, Sequence, Callable\n",
    "from itertools import islice, zip_longest\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2397666",
   "metadata": {},
   "source": [
    "## Origina data transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dad35605",
   "metadata": {},
   "source": [
    "###### Download NQ Train and Dev dataset from https://ai.google.com/research/NaturalQuestions/download\n",
    "###### NQ Train: https://storage.cloud.google.com/natural_questions/v1.0-simplified/simplified-nq-train.jsonl.gz\n",
    "###### NQ Dev: https://storage.cloud.google.com/natural_questions/v1.0-simplified/nq-dev-all.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca748ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7830it [02:20, 55.69it/s]\n"
     ]
    }
   ],
   "source": [
    "nq_dev = []\n",
    "\n",
    "with gzip.open(\"/home/ec2-user/quic-efs/user/bowenjin/seq2seq/retrieval-data/NQ/v1.0-simplified_nq-dev-all.jsonl.gz\", \"r+\") as f:\n",
    "    for item in tqdm(jsonlines.Reader(f)):\n",
    "        \n",
    "        arr = []\n",
    "        ## question_text\n",
    "        question_text = item['question_text']\n",
    "        arr.append(question_text)\n",
    "\n",
    "        tokens = []\n",
    "        for i in item['document_tokens']:\n",
    "            tokens.append(i['token'])\n",
    "        document_text = ' '.join(tokens)\n",
    "        \n",
    "        ## example_id\n",
    "        example_id = str(item['example_id'])\n",
    "        arr.append(example_id)\n",
    "\n",
    "        # document_text = item['document_text']\n",
    "        ## long_answer\n",
    "        annotation = item['annotations'][0]\n",
    "        has_long_answer = annotation['long_answer']['start_token'] >= 0\n",
    "\n",
    "        long_answers = [\n",
    "            a['long_answer']\n",
    "            for a in item['annotations']\n",
    "            if a['long_answer']['start_token'] >= 0 and has_long_answer\n",
    "        ]\n",
    "        if has_long_answer:\n",
    "            start_token = long_answers[0]['start_token']\n",
    "            end_token = long_answers[0]['end_token']\n",
    "            x = document_text.split(' ')\n",
    "            long_answer = ' '.join(x[start_token:end_token])\n",
    "            long_answer = re.sub('<[^<]+?>', '', long_answer).replace('\\n', '').strip()\n",
    "        arr.append(long_answer) if has_long_answer else arr.append('')\n",
    "\n",
    "        # short_answer\n",
    "        has_short_answer = annotation['short_answers'] or annotation['yes_no_answer'] != 'NONE'\n",
    "        short_answers = [\n",
    "            a['short_answers']\n",
    "            for a in item['annotations']\n",
    "            if a['short_answers'] and has_short_answer\n",
    "        ]\n",
    "        if has_short_answer and len(annotation['short_answers']) != 0:\n",
    "            sa = []\n",
    "            for i in short_answers[0]:\n",
    "                start_token_s = i['start_token']\n",
    "                end_token_s = i['end_token']\n",
    "                shorta = ' '.join(x[start_token_s:end_token_s])\n",
    "                sa.append(shorta)\n",
    "            short_answer = '|'.join(sa)\n",
    "            short_answer = re.sub('<[^<]+?>', '', short_answer).replace('\\n', '').strip()\n",
    "        arr.append(short_answer) if has_short_answer else arr.append('')\n",
    "\n",
    "        ## title\n",
    "        arr.append(item['document_title'])\n",
    "\n",
    "        ## abs\n",
    "        if document_text.find('<P>') != -1:\n",
    "            abs_start = document_text.index('<P>')\n",
    "            abs_end = document_text.index('</P>')\n",
    "            abs = document_text[abs_start+3:abs_end]\n",
    "        else:\n",
    "            abs = ''\n",
    "        arr.append(abs)\n",
    "\n",
    "        ## content\n",
    "        if document_text.rfind('</Ul>') != -1:\n",
    "            final = document_text.rindex('</Ul>')\n",
    "            document_text = document_text[:final]\n",
    "            if document_text.rfind('</Ul>') != -1:\n",
    "                final = document_text.rindex('</Ul>')\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "            else:\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "        else:\n",
    "            content = document_text[abs_end+4:]\n",
    "            content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "            content = re.sub(' +', ' ', content)\n",
    "            arr.append(content)\n",
    "        doc_tac = item['document_title'] + abs + content\n",
    "        arr.append(doc_tac)\n",
    "        language = 'en'\n",
    "        arr.append(language)\n",
    "        nq_dev.append(arr)\n",
    "\n",
    "nq_dev_df = pd.DataFrame(nq_dev)\n",
    "nq_dev_df.to_csv(r\"nq_dev.tsv\", sep=\"\\t\", mode = 'w', header=None, index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a93d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_train = []\n",
    "with gzip.open(\"/home/ec2-user/quic-efs/user/bowenjin/seq2seq/retrieval-data/NQ/v1.0-simplified_simplified-nq-train.jsonl.gz\", \"r+\") as f:\n",
    "    for item in jsonlines.Reader(f):\n",
    "        ## question_text\n",
    "        arr = []\n",
    "        question_text = item['question_text']\n",
    "        arr.append(question_text)\n",
    "\n",
    "        ## example_id\n",
    "        example_id = str(item['example_id'])\n",
    "        arr.append(example_id)\n",
    "        \n",
    "        document_text = item['document_text']\n",
    "        \n",
    "        ## long_answer\n",
    "        annotation = item['annotations'][0]\n",
    "        has_long_answer = annotation['long_answer']['start_token'] >= 0\n",
    "\n",
    "        long_answers = [\n",
    "            a['long_answer']\n",
    "            for a in item['annotations']\n",
    "            if a['long_answer']['start_token'] >= 0 and has_long_answer\n",
    "        ]\n",
    "        if has_long_answer:\n",
    "            start_token = long_answers[0]['start_token']\n",
    "            end_token = long_answers[0]['end_token']\n",
    "            x = document_text.split(' ')\n",
    "            long_answer = ' '.join(x[start_token:end_token])\n",
    "            long_answer = re.sub('<[^<]+?>', '', long_answer).replace('\\n', '').strip()\n",
    "        arr.append(long_answer) if has_long_answer else arr.append('')\n",
    "\n",
    "        # short_answer\n",
    "        has_short_answer = annotation['short_answers'] or annotation['yes_no_answer'] != 'NONE'\n",
    "        short_answers = [\n",
    "            a['short_answers']\n",
    "            for a in item['annotations']\n",
    "            if a['short_answers'] and has_short_answer\n",
    "        ]\n",
    "        if has_short_answer and len(annotation['short_answers']) != 0:\n",
    "            sa = []\n",
    "            for i in short_answers[0]:\n",
    "                start_token_s = i['start_token']\n",
    "                end_token_s = i['end_token']\n",
    "                shorta = ' '.join(x[start_token_s:end_token_s])\n",
    "                sa.append(shorta)\n",
    "            short_answer = '|'.join(sa)\n",
    "            short_answer = re.sub('<[^<]+?>', '', short_answer).replace('\\n', '').strip()\n",
    "        arr.append(short_answer) if has_short_answer else arr.append('')\n",
    "\n",
    "        ## title\n",
    "        if document_text.find('<H1>') != -1:\n",
    "            title_start = document_text.index('<H1>')\n",
    "            title_end = document_text.index('</H1>')\n",
    "            title = document_text[title_start+4:title_end]\n",
    "        else:\n",
    "            title = ''\n",
    "        arr.append(title)\n",
    "\n",
    "        ## abs\n",
    "        if document_text.find('<P>') != -1:\n",
    "            abs_start = document_text.index('<P>')\n",
    "            abs_end = document_text.index('</P>')\n",
    "            abs = document_text[abs_start+3:abs_end]\n",
    "        else:\n",
    "            abs = ''\n",
    "        arr.append(abs)\n",
    "\n",
    "        ## content\n",
    "        if document_text.rfind('</Ul>') != -1:\n",
    "            final = document_text.rindex('</Ul>')\n",
    "            document_text = document_text[:final]\n",
    "            if document_text.rfind('</Ul>') != -1:\n",
    "                final = document_text.rindex('</Ul>')\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "            else:\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "        else:\n",
    "            content = document_text[abs_end+4:]\n",
    "            content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "            content = re.sub(' +', ' ', content)\n",
    "            arr.append(content)\n",
    "\n",
    "        doc_tac = title + abs + content\n",
    "        arr.append(doc_tac)\n",
    "\n",
    "        language = 'en'\n",
    "        arr.append(language)\n",
    "        nq_train.append(arr)\n",
    "\n",
    "nq_train_df = pd.DataFrame(nq_train)\n",
    "nq_train_df.to_csv(r\"nq_train.tsv\", sep=\"\\t\", mode = 'w', header=None, index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214437a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mapping tool\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def lower(x):\n",
    "    text = tokenizer.tokenize(x)\n",
    "    id_ = tokenizer.convert_tokens_to_ids(text)\n",
    "    return tokenizer.decode(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e82baca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doc_tac denotes the concatenation of title, abstract and content\n",
    "\n",
    "nq_dev = pd.read_csv('nq_dev.tsv', \\\n",
    "                     names=['query', 'id', 'long_answer', 'short_answer', 'title', 'abstract', 'content', 'doc_tac', 'language'],\\\n",
    "                     header=None, sep='\\t')\n",
    "\n",
    "nq_train = pd.read_csv('nq_train.tsv', \\\n",
    "                       names=['query', 'id', 'long_answer', 'short_answer', 'title', 'abstract', 'content', 'doc_tac', 'language'],\\\n",
    "                       header=None, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9316951",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dev['title'] = nq_dev['title'].map(lower)\n",
    "nq_train['title'] = nq_train['title'].map(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c45cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307373\n",
      "7830\n"
     ]
    }
   ],
   "source": [
    "## Concat train doc and validation doc to obtain full document collection\n",
    "\n",
    "#nq_all_doc = nq_train.append(nq_dev)\n",
    "nq_all_doc = pd.concat([nq_train, nq_dev])\n",
    "nq_all_doc.reset_index(inplace = True)\n",
    "\n",
    "print(len(nq_train))\n",
    "print(len(nq_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8614df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove duplicated documents based on titles\n",
    "\n",
    "nq_all_doc.drop_duplicates('title', inplace = True)\n",
    "nq_all_doc.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f51417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109739"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The total amount of documents : 109739\n",
    "\n",
    "len(nq_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6146320",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct mapping relation\n",
    "\n",
    "title_doc = {}\n",
    "title_doc_id = {}\n",
    "id_doc = {}\n",
    "id_doc_all = {}\n",
    "ran_id_old_id = {}\n",
    "idx = 0\n",
    "doc_id = {}\n",
    "title_id = {}\n",
    "\n",
    "for i in range(len(nq_all_doc)):\n",
    "    title_doc[nq_all_doc['title'][i]] =  nq_all_doc['doc_tac'][i]\n",
    "    title_doc_id[nq_all_doc['title'][i]] = idx\n",
    "    id_doc[idx] = nq_all_doc['doc_tac'][i]\n",
    "    id_doc_all[idx] = {'id': idx, 'title': nq_all_doc['title'][i], 'abstract': nq_all_doc['abstract'][i], 'content': nq_all_doc['content'][i]}\n",
    "    doc_id[nq_all_doc['doc_tac'][i]] = idx\n",
    "\n",
    "    assert nq_all_doc['title'][i] not in title_id\n",
    "    title_id[nq_all_doc['title'][i]] = idx\n",
    "\n",
    "    ran_id_old_id[idx] = nq_all_doc['id'][i]\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276dbb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a461690",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct Document Content File\n",
    "\n",
    "train_file = open(\"corpus.tsv\", 'w') \n",
    "\n",
    "for docid in id_doc.keys():\n",
    "    train_file.write('\\t'.join([str(docid), id_doc[docid]]) + '\\n')\n",
    "    train_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct Document Content File\n",
    "\n",
    "with open(\"corpus.json\", 'w') as train_file:\n",
    "    for docid in id_doc_all.keys():\n",
    "        #train_file.write('\\t'.join([str(docid), id_doc[docid]]) + '\\n')\n",
    "        train_file.write(json.dumps(id_doc_all[docid]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa9888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2112062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109739/109739 [00:05<00:00, 18353.43it/s]\n"
     ]
    }
   ],
   "source": [
    "## Transfer original train/val/test files into good format\n",
    "\n",
    "doc2id = {}\n",
    "id2doc = {}\n",
    "\n",
    "with open(\"corpus.tsv\") as f:\n",
    "    readin = f.readlines()\n",
    "    for line in tqdm(readin):\n",
    "        tmp = line.strip().split('\\t')\n",
    "        assert len(tmp) == 2\n",
    "        assert tmp[1] not in doc2id\n",
    "        doc2id[tmp[1]] = tmp[0]\n",
    "        id2doc[tmp[0]] = tmp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d95d511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307373/307373 [00:03<00:00, 101965.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query with more than 1 ground truth doc: 0/307373\n"
     ]
    }
   ],
   "source": [
    "## process train\n",
    "\n",
    "train_dict = defaultdict(set)\n",
    "for iid in tqdm(range(len(nq_train))):\n",
    "    train_dict[nq_train['query'][iid]].add(title_id[nq_train['title'][iid]])\n",
    "\n",
    "stats = 0\n",
    "with open('train.csv', 'w') as fout:\n",
    "    for query in train_dict:\n",
    "        if len(train_dict[query]) > 1:\n",
    "            stats += 1\n",
    "        fout.write(query + '\\t' + ','.join([str(ii) for ii in list(train_dict[query])]) + '\\n')\n",
    "\n",
    "print(f'query with more than 1 ground truth doc: {stats}/{len(train_dict)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33b2ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7830/7830 [00:00<00:00, 109717.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query with more than 1 ground truth doc: 0/7830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## process test\n",
    "\n",
    "dev_dict = defaultdict(set)\n",
    "for iid in tqdm(range(len(nq_dev))):\n",
    "    dev_dict[nq_dev['query'][iid]].add(title_id[nq_dev['title'][iid]])\n",
    "\n",
    "stats = 0\n",
    "with open('test.csv', 'w') as fout:\n",
    "    for query in dev_dict:\n",
    "        if len(dev_dict[query]) > 1:\n",
    "            stats += 1\n",
    "        fout.write(query + '\\t' + ','.join([str(ii) for ii in list(dev_dict[query])]) + '\\n')\n",
    "\n",
    "print(f'query with more than 1 ground truth doc: {stats}/{len(dev_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a096c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
